{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {}\n",
    "\n",
    "properties['spark.app.name'] = \"1B-kafka-postgres-stream\"\n",
    "\n",
    "properties['spark.sql.streaming.checkpointLocation'] = f\"hdfs://172.21.1.190:8020/var/workspace/spark/app/checkpoint/{properties['spark.app.name']}\"\n",
    "\n",
    "properties['spark.stream.read.0A.kafka.bootstrap.servers'] = \"172.21.1.110:39092\"\n",
    "properties['spark.stream.read.0A.subscribe']               = \"kc_tradingexpert_fixlogtracer_ullink_ulbridge_bridge_v1\"\n",
    "properties['spark.stream.read.0A.startingOffsets']         = \"earliest\"\n",
    "properties['spark.stream.read.0A.maxOffsetsPerTrigger']    = 10000\n",
    "properties['spark.stream.read.0A.kafka.group.id']          = f\"{properties['spark.app.name']}\"\n",
    "\n",
    "properties['spark.stream.write.1A.kafka.bootstrap.servers'] = \"172.21.1.110:39092\"\n",
    "properties['spark.stream.write.1A.topic']                   = f\"{properties['spark.app.name']}\"\n",
    "\n",
    "properties['spark.stream.write.1B.url']                     = \"jdbc:postgresql://172.21.1.110:5432/kc_tradingexpert_fixorderlinker\"\n",
    "properties['spark.stream.write.1B.dbtable']                 = f\"{properties['spark.app.name']}\"\n",
    "properties['spark.stream.write.1B.user']                    = \"admin\"\n",
    "properties['spark.stream.write.1B.password']                = \"adminadmin123456\"\n",
    "\n",
    "\n",
    "properties['spark.driver.memory']                        = \"512m\"\n",
    "\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kc.annotation.connectivity import ConnectivitySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ConnectivitySpark(configuration={\n",
    "    \"spark.app.name\": properties['spark.app.name'],\n",
    "    \"spark.sql.streaming.checkpointLocation\": properties['spark.sql.streaming.checkpointLocation'],\n",
    "    \"spark.driver.memory\": properties['spark.driver.memory']\n",
    "})\n",
    "class Application:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6296cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Application()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d366b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source = app.spark.readStream.format(\"kafka\") \\\n",
    "                 .option(\"kafka.bootstrap.servers\", properties[\"spark.stream.read.0A.kafka.bootstrap.servers\"]) \\\n",
    "                 .option(\"subscribe\", properties[\"spark.stream.read.0A.subscribe\"]) \\\n",
    "                 .option(\"startingOffsets\", properties[\"spark.stream.read.0A.startingOffsets\"]) \\\n",
    "                 .option(\"maxOffsetsPerTrigger\", properties[\"spark.stream.read.0A.maxOffsetsPerTrigger\"]) \\\n",
    "                 .option(\"kafka.group.id\", properties[\"spark.stream.read.0A.kafka.group.id\"]) \\\n",
    "                 .load() \\\n",
    "\n",
    "df_source.createOrReplaceTempView(\"df_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc31e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = app.spark.sql(\"\"\"\\\n",
    "----------------------\n",
    "SELECT  \n",
    "      CAST(value AS STRING) AS json\n",
    "    , timestamp             AS _KAFKA_SOURCE_MESSAGE_TIMESTAMP\n",
    "\n",
    "FROM df_source\n",
    "----------------------\n",
    "\"\"\")\n",
    "\n",
    "df0.createOrReplaceTempView(\"df0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12feecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = app.spark.sql(\"\"\"\\\n",
    "----------------------\n",
    "SELECT\n",
    "    from_json(json, 'struct< `metadata`   : struct< `namespace` : string \n",
    "                                                   , `name`      : string\n",
    "                                                   , `name0`     : string \n",
    "                                                   , `size0`     : int\n",
    "                                                   , `message0`  : struct< `id`           : string\n",
    "                                                                         , `type`         : string\n",
    "                                                                         , `timestamp`    : string\n",
    "                                                                         , `date`         : string\n",
    "                                                                         , `time`         : string\n",
    "                                                                         , `process_name` : string\n",
    "                                                                         , `loglevel`     : string\n",
    "                                                                         , `process_id`   : string\n",
    "                                                                         >\n",
    "                                                   >\n",
    "                            , `spec`       : struct< `fix`                  : string\n",
    "                                                   , `source_process_name`  : string\n",
    "                                                   , `sink_process_name`    : string\n",
    "                                                   , `source_CLORDID`       : string\n",
    "                                                   , `sink_CLORDID`         : string\n",
    "                                                   , `action`               : string\n",
    "                            >\n",
    "                            , `@timestamp` : timestamp\n",
    "                            >\n",
    "                     ')               AS dict\n",
    "    , _KAFKA_SOURCE_MESSAGE_TIMESTAMP\n",
    "\n",
    "FROM df0\n",
    "----------------------\n",
    "\"\"\")\n",
    "\n",
    "df1.createOrReplaceTempView(\"df1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e53eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = app.spark.sql(\"\"\"\\\n",
    "----------------------\n",
    "SELECT\n",
    "      dict.*\n",
    "    , _KAFKA_SOURCE_MESSAGE_TIMESTAMP\n",
    "\n",
    "FROM df1\n",
    "----------------------\n",
    "\"\"\")\n",
    "\n",
    "df2.createOrReplaceTempView(\"df2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903cd9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "\n",
    "def __DumpBatchDF_SinkGroup_2(batch_DF: pyspark.sql.DataFrame, batch_id: int) -> None:\n",
    "    batch_DF.persist()\n",
    "    batch_DF.write.format(\"jdbc\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .option(\"url\", properties['spark.stream.write.1B.url']) \\\n",
    "            .option(\"dbtable\", properties['spark.stream.write.1B.dbtable']) \\\n",
    "            .option(\"user\", properties['spark.stream.write.1B.user']) \\\n",
    "            .option(\"password\", properties['spark.stream.write.1B.password']) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "    batch_DF.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecb044",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = df2.writeStream \\\n",
    "                  .foreachBatch(__DumpBatchDF_SinkGroup_2) \\\n",
    "                  .outputMode(\"append\") \\\n",
    "                  .trigger(processingTime='1 second') \\\n",
    "                  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff886a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keplercheuvreux",
   "language": "python",
   "name": "keplercheuvreux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
