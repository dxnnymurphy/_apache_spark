{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafed1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {}\n",
    "\n",
    "properties['spark.app.name'] = \"1A-kafka-kafka-stream\"\n",
    "\n",
    "properties['spark.sql.streaming.checkpointLocation'] = f\"hdfs://172.21.1.190:8020/var/workspace/spark/app/checkpoint/{properties['spark.app.name']}\"\n",
    "\n",
    "properties['spark.stream.read.0A.kafka.bootstrap.servers'] = \"172.21.1.110:39092\"\n",
    "properties['spark.stream.read.0A.subscribe']               = \"kc_tradingexpert_fixlogtracer_ullink_ulbridge_bridge_v1\"\n",
    "properties['spark.stream.read.0A.startingOffsets']         = \"earliest\"\n",
    "properties['spark.stream.read.0A.maxOffsetsPerTrigger']    = 10000\n",
    "properties['spark.stream.read.0A.kafka.group.id']          = f\"{properties['spark.app.name']}\"\n",
    "\n",
    "properties['spark.stream.write.1A.kafka.bootstrap.servers'] = \"172.21.1.110:39092\"\n",
    "properties['spark.stream.write.1A.topic']                   = f\"{properties['spark.app.name']}\"\n",
    "\n",
    "\n",
    "properties['spark.driver.memory']                        = \"512m\"\n",
    "\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70262cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kc.annotation.connectivity import ConnectivitySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa71c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ConnectivitySpark(configuration={\n",
    "    \"spark.app.name\": properties['spark.app.name'],\n",
    "    \"spark.sql.streaming.checkpointLocation\": properties['spark.sql.streaming.checkpointLocation'],\n",
    "    \"spark.driver.memory\": properties['spark.driver.memory']\n",
    "})\n",
    "class Application:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Application()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7165f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef38080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source = app.spark.readStream.format(\"kafka\") \\\n",
    "                 .option(\"kafka.bootstrap.servers\", properties[\"spark.stream.read.0A.kafka.bootstrap.servers\"]) \\\n",
    "                 .option(\"subscribe\", properties[\"spark.stream.read.0A.subscribe\"]) \\\n",
    "                 .option(\"startingOffsets\", properties[\"spark.stream.read.0A.startingOffsets\"]) \\\n",
    "                 .option(\"maxOffsetsPerTrigger\", properties[\"spark.stream.read.0A.maxOffsetsPerTrigger\"]) \\\n",
    "                 .option(\"kafka.group.id\", properties[\"spark.stream.read.0A.kafka.group.id\"]) \\\n",
    "                 .load() \\\n",
    "\n",
    "df_source.createOrReplaceTempView(\"df_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf43c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = app.spark.sql(\"\"\"\\\n",
    "----------------------\n",
    "SELECT  \n",
    "      CAST(value AS STRING) AS json\n",
    "    , timestamp             AS _KAFKA_SOURCE_MESSAGE_TIMESTAMP\n",
    "\n",
    "FROM df_source\n",
    "----------------------\n",
    "\"\"\")\n",
    "\n",
    "df0.createOrReplaceTempView(\"df0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = app.spark.sql(\"\"\"\\\n",
    "----------------------\n",
    "SELECT\n",
    "    from_json(json, 'struct< `metadata`   : struct< `namespace` : string \n",
    "                                                   , `name`      : string\n",
    "                                                   , `name0`     : string \n",
    "                                                   , `size0`     : int\n",
    "                                                   , `message0`  : struct< `id`           : string\n",
    "                                                                         , `type`         : string\n",
    "                                                                         , `timestamp`    : string\n",
    "                                                                         , `date`         : string\n",
    "                                                                         , `time`         : string\n",
    "                                                                         , `process_name` : string\n",
    "                                                                         , `loglevel`     : string\n",
    "                                                                         , `process_id`   : string\n",
    "                                                                         >\n",
    "                                                   >\n",
    "                            , `spec`       : struct< `fix`                  : string\n",
    "                                                   , `source_process_name`  : string\n",
    "                                                   , `sink_process_name`    : string\n",
    "                                                   , `source_CLORDID`       : string\n",
    "                                                   , `sink_CLORDID`         : string\n",
    "                                                   , `action`               : string\n",
    "                            >\n",
    "                            , `@timestamp` : timestamp\n",
    "                            >\n",
    "                     ')               AS dict\n",
    "    , _KAFKA_SOURCE_MESSAGE_TIMESTAMP\n",
    "\n",
    "FROM df0\n",
    "----------------------\n",
    "\"\"\")\n",
    "\n",
    "df1.createOrReplaceTempView(\"df1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf0ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773c3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = app.spark.sql(\"\"\"\\\n",
    "----------------------\n",
    "SELECT\n",
    "      dict.*\n",
    "    , _KAFKA_SOURCE_MESSAGE_TIMESTAMP\n",
    "\n",
    "FROM df1\n",
    "----------------------\n",
    "\"\"\")\n",
    "\n",
    "df2.createOrReplaceTempView(\"df2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bd852",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "\n",
    "def __DumpBatchDF_SinkGroup_2(batch_DF: pyspark.sql.DataFrame, batch_id: int) -> None:\n",
    "    batch_DF.persist()\n",
    "    batch_DF.selectExpr(\"to_json(struct(*)) AS value\") \\\n",
    "            .write.format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", properties['spark.stream.write.1A.kafka.bootstrap.servers']) \\\n",
    "            .option(\"topic\", properties['spark.stream.write.1A.topic']) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "    batch_DF.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = df2.writeStream \\\n",
    "                  .foreachBatch(__DumpBatchDF_SinkGroup_2) \\\n",
    "                  .outputMode(\"append\") \\\n",
    "                  .trigger(processingTime='1 second') \\\n",
    "                  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keplercheuvreux",
   "language": "python",
   "name": "keplercheuvreux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
